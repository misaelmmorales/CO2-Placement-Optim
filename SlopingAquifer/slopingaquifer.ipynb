{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import Model\n",
    "from keras.layers import Layer\n",
    "from keras.callbacks import Callback\n",
    "from keras import layers, regularizers, optimizers, losses, metrics, callbacks\n",
    "\n",
    "NUM_REALIZATIONS = 1272\n",
    "X_CHANNELS = 4\n",
    "Y_CHANNELS = 2\n",
    "NX  = 160\n",
    "NY  = 160\n",
    "NZ  = 5\n",
    "NTT = 60\n",
    "NT1 = 30\n",
    "NT0 = 24\n",
    "\n",
    "sec2year   = 365.25 * 24 * 60 * 60\n",
    "Darcy      = 9.869233e-13\n",
    "psi2pascal = 6894.76\n",
    "co2_rho    = 686.5266\n",
    "milli      = 1e-3\n",
    "mega       = 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "----------------------- VERSION INFO -----------------------\n",
      "TF version: 2.15.0 | # Device(s) available: 2\n",
      "TF Built with CUDA? True | CUDA: 12.2 | cuDNN: 8\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_tf_gpu():\n",
    "    sys_info = tf.sysconfig.get_build_info()\n",
    "    version, cuda, cudnn = tf.__version__, sys_info[\"cuda_version\"], sys_info[\"cudnn_version\"]\n",
    "    count = len(tf.config.experimental.list_physical_devices())\n",
    "    name  = [device.name for device in tf.config.experimental.list_physical_devices('GPU')]\n",
    "    print('-'*60)\n",
    "    print('----------------------- VERSION INFO -----------------------')\n",
    "    print('TF version: {} | # Device(s) available: {}'.format(version, count))\n",
    "    print('TF Built with CUDA? {} | CUDA: {} | cuDNN: {}'.format(tf.test.is_built_with_cuda(), cuda, cudnn))\n",
    "    print(tf.config.list_physical_devices()[-1])\n",
    "    print('-'*60+'\\n')\n",
    "    return None\n",
    "\n",
    "check_tf_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridVE = sio.loadmat('Gt.mat', simplify_cells=True)['Gt']\n",
    "tops2d = -gridVE['cells']['z'].reshape(NX,NY)\n",
    "tops3d = -sio.loadmat('G.mat', simplify_cells=True)['G']['cells']['centroids'].reshape(NX,NY,NZ,3, order='F')[...,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_data = np.load('data/X_data.npy')\n",
    "c_data = np.load('data/c_data.npy')\n",
    "y1_data = np.load('data/y1_data.npy')\n",
    "y2_data = np.load('data/y2_data.npy')\n",
    "timesteps = np.load('data/timesteps.npy')\n",
    "print('X: {} | c: {}'.format(X_data.shape, c_data.shape))\n",
    "print('y1: {} | y2: {}'.format(y1_data.shape, y2_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "X_data[...,0] = (X_data[...,0] - 0.10) / (0.32 - 0.10)\n",
    "X_data[...,1] = np.log10(X_data[...,1] / (milli*Darcy))\n",
    "X_data[...,1] = (X_data[...,1] + 0.26) / (4.0 + 0.26)\n",
    "X_data[...,2] = (X_data[...,2] - X_data[...,2].min()) / (X_data[...,2].max() - X_data[...,2].min())\n",
    "c_data = c_data / 10\n",
    "y1_data[...,0] = y1_data[...,0] / psi2pascal / 127e3\n",
    "y2_data = y2_data[...,-1]\n",
    "print('X: {} | c: {}'.format(X_data.shape, c_data.shape))\n",
    "print('y1: {} | y2: {}'.format(y1_data.shape, y2_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.random.choice(range(len(X_data)), 1000, replace=False)\n",
    "test_idx  = np.setdiff1d(range(len(X_data)), train_idx)\n",
    "\n",
    "X_train = X_data[train_idx]\n",
    "c_train = c_data[train_idx]\n",
    "y1_train = y1_data[train_idx]\n",
    "y2_train = y2_data[train_idx]\n",
    "\n",
    "X_test = X_data[test_idx]\n",
    "c_test = c_data[test_idx]\n",
    "y1_test = y1_data[test_idx]\n",
    "y2_test = y2_data[test_idx]\n",
    "\n",
    "print('X_train:  {}     | c_train: {}'.format(X_train.shape, c_train.shape))\n",
    "print('y1_train: {} | y2_train: {}'.format(y1_train.shape, y2_train.shape))\n",
    "print('-'*70)\n",
    "print('X_test:  {}     | c_test: {}'.format(X_test.shape, c_test.shape))\n",
    "print('y1_test: {} | y2_test: {}'.format(y1_test.shape, y2_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcite(Layer):\n",
    "    def __init__(self, ratio=4, **kwargs):\n",
    "        super(SqueezeExcite, self).__init__(**kwargs)\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        self.squeeze = layers.GlobalAveragePooling2D()\n",
    "        self.excite1 = layers.Dense(channels // self.ratio, activation='relu')\n",
    "        self.excite2 = layers.Dense(channels, activation='sigmoid')\n",
    "        super(SqueezeExcite, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.squeeze(inputs)\n",
    "        x = self.excite1(x)\n",
    "        x = self.excite2(x)\n",
    "        x = layers.Reshape((1, 1, x.shape[-1]))(x)\n",
    "        s = layers.Multiply()([inputs, x])\n",
    "        return layers.Add()([inputs, s])\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(inp, filt, k=3, pad='same', drop=0.1, pool=(2,2)):\n",
    "    _ = layers.Conv2D(filt, k, activity_regularizer=regularizers.l1(1e-6), padding=pad)(inp)\n",
    "    _ = SqueezeExcite()(_)\n",
    "    _ = layers.GroupNormalization(groups=-1)(_)\n",
    "    _ = layers.PReLU()(_)\n",
    "    _ = layers.MaxPooling2D(pool)(_)\n",
    "    #_ = layers.SpatialDropout2D(drop)(_)\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lifting_layer(inp, dim, drop=0.1):\n",
    "    _ = layers.Dense(dim)(inp)\n",
    "    _ = layers.Activation('gelu')(_)\n",
    "    #_ = layers.Dropout(drop)(_)\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrent_step(inp, filt, res, kern=3, pad='same', drop=0.0, leaky_slope=0.3):\n",
    "    y = layers.ConvLSTM2D(filt, kern, padding=pad)(inp)\n",
    "    y = layers.GroupNormalization(groups=-1)(y)\n",
    "    y = layers.LeakyReLU(leaky_slope)(y)\n",
    "    y = layers.Conv2DTranspose(filt, kern, padding=pad, strides=2)(y)\n",
    "    #y = layers.SpatialDropout2D(drop)(y)\n",
    "    y = layers.Concatenate()([y, res])\n",
    "    y = layers.Conv2D(filt, kern, padding=pad)(y)\n",
    "    y = layers.Activation('sigmoid')(y)\n",
    "    y = tf.expand_dims(y,1)\n",
    "    return y\n",
    "\n",
    "def recurrent_last(inp, filt, kern=3, pad='same', drop=0.0, leaky_slope=0.3, out_channels=2):\n",
    "    y = layers.ConvLSTM2D(filt, kern, padding=pad)(inp)\n",
    "    y = layers.GroupNormalization(groups=-1)(y)\n",
    "    y = layers.LeakyReLU(leaky_slope)(y)\n",
    "    y = layers.Conv2DTranspose(filt, kern, padding=pad, strides=2)(y)\n",
    "    #y = layers.SpatialDropout2D(drop)(y)\n",
    "    y = layers.Conv2D(out_channels, kern, padding=pad)(y)\n",
    "    y = layers.Activation('sigmoid')(y)\n",
    "    y = tf.expand_dims(y, 1)\n",
    "    return y\n",
    "\n",
    "def conditional_recurrent_decoder(z_input, c_input, residuals, rnn_filters=[16,64,256], \n",
    "                                  previous_timestep=None, dropout=0.1, leaky_slope=0.3, out_channels:int=2):\n",
    "    zz = tf.expand_dims(z_input, 1)\n",
    "    cc = tf.expand_dims(c_input, 1)\n",
    "    _ = tf.einsum('bthwc,btc->bthwc', zz, cc)\n",
    "    _ = recurrent_step(_, rnn_filters[0], residuals[0], drop=dropout, leaky_slope=leaky_slope)\n",
    "    _ = recurrent_step(_, rnn_filters[1], residuals[1], drop=dropout, leaky_slope=leaky_slope)\n",
    "    _ = recurrent_last(_, rnn_filters[2], drop=dropout, leaky_slope=leaky_slope, out_channels=out_channels)\n",
    "    if previous_timestep is not None:\n",
    "        _ = layers.Concatenate(axis=1)([previous_timestep, _])\n",
    "    return _\n",
    "\n",
    "def unconditional_recurrent_decoder(z_input, residuals, rnn_filters=[16,64,256], \n",
    "                                    previous_timestep=None, dropout=0.1, leaky_slope=0.3, out_channels:int=2):    \n",
    "    _ = tf.expand_dims(z_input, 1)\n",
    "    _ = recurrent_step(_, rnn_filters[0], residuals[0], drop=dropout, leaky_slope=leaky_slope)\n",
    "    _ = recurrent_step(_, rnn_filters[1], residuals[1], drop=dropout, leaky_slope=leaky_slope)\n",
    "    _ = recurrent_last(_, rnn_filters[2], drop=dropout, leaky_slope=leaky_slope, out_channels=out_channels)\n",
    "    if previous_timestep is not None:\n",
    "        _ = layers.Concatenate(axis=1)([previous_timestep, _])\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(timesteps=30, verbose:bool=True):\n",
    "    x_inp = layers.Input(shape=(NX, NY, X_CHANNELS))\n",
    "    c_inp = layers.Input(shape=(timesteps, 5))\n",
    "\n",
    "    x1 = encoder_layer(x_inp, 16)\n",
    "    x2 = encoder_layer(x1, 64)\n",
    "    x3 = encoder_layer(x2, 256)\n",
    "    cc = lifting_layer(c_inp, 256)\n",
    "    t1 = None\n",
    "    for t in range(timesteps):\n",
    "        if t==0:\n",
    "            t1 = conditional_recurrent_decoder(x3, cc[:,t], [x2, x1])\n",
    "        else:\n",
    "            t1 = conditional_recurrent_decoder(x3, cc[:,t], [x2, x1], previous_timestep=t1) \n",
    "\n",
    "    model = Model(inputs=[x_inp, c_inp], outputs=t1)\n",
    "\n",
    "    if verbose:\n",
    "        print('# parameters: {:,}'.format(model.count_params()))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(true, pred, alpha=0.8, beta=0.8, gamma=0.8):\n",
    "\n",
    "    # perceptual loss\n",
    "    ssim_loss = tf.reduce_mean(1.0 - tf.image.ssim(true, pred, max_val=1.0))\n",
    "    psnr_loss = tf.reduce_mean(1.0 / tf.image.psnr(true, pred, max_val=1.0))\n",
    "    perceptual = gamma * ssim_loss + (1 - gamma) * psnr_loss\n",
    "    \n",
    "    # reconstruction loss\n",
    "    mse_loss = tf.reduce_mean(tf.square(true - pred))\n",
    "    mae_loss = tf.reduce_mean(tf.abs(true - pred))\n",
    "    reconstruction = beta * mse_loss + (1 - beta) * mae_loss\n",
    "    \n",
    "    # total loss\n",
    "    return alpha * reconstruction + (1 - alpha) * perceptual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonitorCallback(Callback):\n",
    "    def __init__(self, monitor:int=10):\n",
    "        super(MonitorCallback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch+1) % self.monitor == 0:\n",
    "            print('Epoch: {} | Loss: {:.5f} | Val Loss: {:.5f}'.format(epoch+1, logs['loss'], logs['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "optimizer = keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-6)\n",
    "model.compile(optimizer=optimizer, loss=custom_loss, metrics=['mse'])\n",
    "\n",
    "esCallback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\n",
    "mcCallback = keras.callbacks.ModelCheckpoint('pix2vid-opt.keras', monitor='val_accuracy', save_best_only=True)\n",
    "customCBs  = [MonitorCallback(monitor=10)] #, esCallback, mcCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "fit = model.fit(x=[X_train, c_train], y=[y1_train],\n",
    "                batch_size       = 4,\n",
    "                epochs           = 100,\n",
    "                validation_split = 0.2,\n",
    "                shuffle          = True,\n",
    "                callbacks        = customCBs,\n",
    "                verbose          = 1)\n",
    "print('-'*30+'\\n'+'Training time: {:.2f} minutes'.format((time()-start)/60))\n",
    "model.save('pix2vid-opt.keras')\n",
    "pd.DataFrame(fit.history).to_csv('pix2vid-opt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
