{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from skimage.metrics import mean_squared_error, structural_similarity, peak_signal_noise_ratio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import check_torch, pix2vid_dataset, DualCustomLoss, DualLpLoss, NeuralPix2Vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "----------------------- VERSION INFO -----------------------\n",
      "Torch version: 2.3.1.post300 | Torch Built with CUDA? True\n",
      "# Device(s) available: 1, Name(s): NVIDIA GeForce RTX 3090\n",
      "Torch device: cuda\n",
      "------------------------------------------------------------\n",
      "timesteps: 60 | deltaT: [ 1. 50.]\n",
      "Train - X:  torch.Size([1000, 4, 64, 64])     | c:  torch.Size([1000, 30, 5])\n",
      "        y1: torch.Size([1000, 30, 2, 64, 64]) | y2: torch.Size([1000, 30, 1, 64, 64])\n",
      "--------------------\n",
      "Valid - X:  torch.Size([136, 4, 64, 64])     | c:  torch.Size([136, 30, 5])\n",
      "        y1: torch.Size([136, 30, 2, 64, 64]) | y2: torch.Size([136, 30, 1, 64, 64])\n",
      "--------------------\n",
      "Test  - X:  torch.Size([136, 4, 64, 64])     | c:  torch.Size([136, 30, 5])\n",
      "        y1: torch.Size([136, 30, 2, 64, 64]) | y2: torch.Size([136, 30, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "NR, NT = 1272, 60\n",
    "NX, NY = 64, 64\n",
    "milli  = 1e-3\n",
    "mega   = 1e6\n",
    "Darcy  = 9.869233e-13\n",
    "psi2pa = 6894.75729\n",
    "co2rho = 686.5400\n",
    "sec2yr = 1/(3600*24*365.25)\n",
    "device = check_torch()\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "timesteps = np.load('timesteps.npy')\n",
    "deltaTime = np.concatenate([np.ones((30)), np.ones((30))*50])\n",
    "print('timesteps: {} | deltaT: {}'.format(len(timesteps), np.unique(deltaTime)))\n",
    "(Xt, ct, y1t, y2t, all_volumes, idx), (trainloader, validloader) = pix2vid_dataset(send_to_device=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# parameters: 4,261,815 | device: cpu\n"
     ]
    }
   ],
   "source": [
    "model = NeuralPix2Vid(device=device).to(device)\n",
    "nparams = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('# parameters: {:,} | device: {}'.format(nparams, model.device))\n",
    "criterion = DualLpLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x,c,y1,y2) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader):\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 9\u001b[0m     u1, u2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y1, y2, u1, u2)\n\u001b[1;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/rapid/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapid/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/e/Placement-Control-Optim-CO2/SlopingAquiferSmall/utils.py:262\u001b[0m, in \u001b[0;36mNeuralPix2Vid.forward\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m    260\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_decoder_layer(z3, c, [z2, z1], \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcond_decoder_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mz2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m y1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout1(y\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Encoder (post-injection)\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/e/Placement-Control-Optim-CO2/SlopingAquiferSmall/utils.py:230\u001b[0m, in \u001b[0;36mNeuralPix2Vid.cond_decoder_layer\u001b[0;34m(self, inputs, controls, residuals, previous_step)\u001b[0m\n\u001b[1;32m    228\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbtchw, bkcpq -> btchw\u001b[39m\u001b[38;5;124m'\u001b[39m, inputs, c)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# spatiotemporal\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresiduals\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m(x)\n\u001b[1;32m    231\u001b[0m x \u001b[38;5;241m=\u001b[39m DecoderLayer(hidden[\u001b[38;5;241m1\u001b[39m], hidden[\u001b[38;5;241m0\u001b[39m], residuals[\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)(x)\n\u001b[1;32m    232\u001b[0m x \u001b[38;5;241m=\u001b[39m DecoderLayer(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m4\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)(x)\n",
      "File \u001b[0;32m/mnt/e/Placement-Control-Optim-CO2/SlopingAquiferSmall/utils.py:189\u001b[0m, in \u001b[0;36mDecoderLayer.__init__\u001b[0;34m(self, in_channels, out_channels, residual, n_modes, kernel_size, stride, padding, num_layers, batch_first, bias, return_all_layers, device)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m#self.upsm     = nn.ConvTranspose3d(out_channels, out_channels, tuple(kernel_size+[2]), stride+1, padding, output_padding=padding, groups=out_channels)\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m#self.conv     = nn.Conv3d(out_channels, out_channels, tuple(kernel_size+[3]), stride, padding, groups=out_channels)\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsm     \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mUpsample(scale_factor\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv     \u001b[38;5;241m=\u001b[39m \u001b[43mSpectralConv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_modes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_modes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual \u001b[38;5;241m=\u001b[39m residual\n",
      "File \u001b[0;32m~/miniconda3/envs/rapid/lib/python3.11/site-packages/neuralop/layers/spectral_convolution.py:353\u001b[0m, in \u001b[0;36mSpectralConv.__init__\u001b[0;34m(self, in_channels, out_channels, n_modes, max_n_modes, bias, n_layers, separable, output_scaling_factor, fno_block_precision, rank, factorization, implementation, fixed_rank_modes, joint_factorization, decomposition_kwargs, init_std, fft_norm, device, dtype)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    341\u001b[0m         [\n\u001b[1;32m    342\u001b[0m             FactorizedTensor\u001b[38;5;241m.\u001b[39mnew(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m         ]\n\u001b[1;32m    351\u001b[0m     )\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight:\n\u001b[0;32m--> 353\u001b[0m         \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_contract \u001b[38;5;241m=\u001b[39m get_contract_fun(\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight[\u001b[38;5;241m0\u001b[39m], implementation\u001b[38;5;241m=\u001b[39mimplementation, separable\u001b[38;5;241m=\u001b[39mseparable\n\u001b[1;32m    356\u001b[0m )\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n",
      "File \u001b[0;32m~/miniconda3/envs/rapid/lib/python3.11/site-packages/tltorch/factorized_tensors/factorized_tensors.py:62\u001b[0m, in \u001b[0;36mDenseTensor.normal_\u001b[0;34m(self, mean, std)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormal_\u001b[39m(\u001b[38;5;28mself\u001b[39m, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "epochs, monitor = 10, 1\n",
    "train_loss, valid_loss = [], []\n",
    "for epoch in range(epochs):\n",
    "    epoch_train_loss = []\n",
    "    model.train()\n",
    "    for i, (x,c,y1,y2) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        u1, u2 = model(x,c)\n",
    "        loss = criterion(y1, y2, u1, u2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss.append(loss.item())\n",
    "    train_loss.append(np.mean(epoch_train_loss))\n",
    "    # validation\n",
    "    model.eval()\n",
    "    epoch_valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        for i, (x,c,y1,y2) in enumerate(validloader):\n",
    "            u1, u2 = model(x,c)\n",
    "            loss = criterion(y1, y2, u1, u2)\n",
    "            epoch_valid_loss.append(loss.item())\n",
    "    valid_loss.append(np.mean(epoch_valid_loss))\n",
    "    # progress\n",
    "    if (epoch+1) % monitor == 0:\n",
    "        print('Epoch: [{}/{}] | Train Loss: {:.5f} | Valid Loss: {:.5f}'.format(epoch+1, epochs, train_loss[-1], valid_loss[-1]))\n",
    "\n",
    "print('Total training time: {:.3f} minuts'.format((time()-start)/60))\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "losses = pd.DataFrame({'train': train_loss, 'valid': valid_loss})\n",
    "losses.to_csv('losses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
