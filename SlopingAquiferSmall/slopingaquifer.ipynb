{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import Model\n",
    "from keras.layers import Layer\n",
    "from keras.callbacks import Callback\n",
    "from keras import layers, regularizers, optimizers, losses, metrics, callbacks\n",
    "\n",
    "NUM_REALIZATIONS = 929\n",
    "X_CHANNELS = 6\n",
    "Y_CHANNELS = 2\n",
    "NX  = 64\n",
    "NY  = 64\n",
    "NZ  = 5\n",
    "NTT = 40\n",
    "NT0 = 20\n",
    "\n",
    "sec2year   = 365.25 * 24 * 60 * 60\n",
    "Darcy      = 9.869233e-13\n",
    "psi2pascal = 6894.76\n",
    "co2_rho    = 686.5266\n",
    "milli      = 1e-3\n",
    "mega       = 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tf_gpu():\n",
    "    sys_info = tf.sysconfig.get_build_info()\n",
    "    version, cuda, cudnn = tf.__version__, sys_info[\"cuda_version\"], sys_info[\"cudnn_version\"]\n",
    "    count = len(tf.config.experimental.list_physical_devices())\n",
    "    name  = [device.name for device in tf.config.experimental.list_physical_devices('GPU')]\n",
    "    print('-'*60)\n",
    "    print('----------------------- VERSION INFO -----------------------')\n",
    "    print('TF version: {} | # Device(s) available: {}'.format(version, count))\n",
    "    print('TF Built with CUDA? {} | CUDA: {} | cuDNN: {}'.format(tf.test.is_built_with_cuda(), cuda, cudnn))\n",
    "    print(tf.config.list_physical_devices()[-1])\n",
    "    print('-'*60+'\\n')\n",
    "    return None\n",
    "\n",
    "check_tf_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltatime = sio.loadmat('simulations/data/time_arr.mat', simplify_cells=True)['time_arr']\n",
    "timesteps = np.cumsum(deltatime)\n",
    "print('timesteps: {} | deltatime: {}'.format(len(timesteps), np.unique(deltatime)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_data = np.load('simulations/data/X_data.npy')\n",
    "c_data = np.load('simulations/data/c_data.npy')\n",
    "y1_data = np.load('simulations/data/y1_data.npy')\n",
    "y2_data = np.load('simulations/data/y2_data.npy')\n",
    "print('X: {} | c: {}'.format(X_data.shape, c_data.shape))\n",
    "print('y1: {} | y2: {}'.format(y1_data.shape, y2_data.shape))\n",
    "\n",
    "# Normalize data\n",
    "pmu, psd = X_data[...,0].mean(), X_data[...,0].std() # porosity\n",
    "kmu, ksd = X_data[...,1].mean(), X_data[...,1].std() # permeability\n",
    "vmu, vsd = X_data[...,3].mean(), X_data[...,3].std() # poreVol\n",
    "tmi, tma = X_data[...,4].min(),  X_data[...,4].max() # tops\n",
    "hmi, hma = X_data[...,5].min(),  X_data[...,5].max() # heights\n",
    "cmi, cma = c_data.min(),         c_data.max()        # controls\n",
    "\n",
    "X_data[...,0] = (X_data[...,0] - pmu) / (3.33*psd)\n",
    "X_data[...,1] = (X_data[...,1] - kmu) / (3.33*ksd)\n",
    "X_data[...,3] = (X_data[...,3] - vmu) / (3.33*vsd)\n",
    "X_data[...,4] = (X_data[...,4] - tmi) / (tma - tmi)\n",
    "X_data[...,5] = (X_data[...,5] - hmi) / (hma - hmi)\n",
    "c_data = c_data / 2.0\n",
    "\n",
    "y1_data[...,0]  = y1_data[...,0]  / 50e3\n",
    "y1_data[...,-1] = y1_data[...,-1] / 0.73\n",
    "y2_data[...,-1] = y2_data[...,-1] / 0.73\n",
    "\n",
    "print('porosity     - min: {:.2f} | max: {:.2f}'.format(X_data[...,0].min(), X_data[...,0].max()))\n",
    "print('logperm      - min: {:.2f} | max: {:.2f}'.format(X_data[...,1].min(), X_data[...,1].max()))\n",
    "print('poreVol      - min: {:.2f} | max: {:.2f}'.format(X_data[...,3].min(), X_data[...,3].max()))\n",
    "print('tops         - min: {:.2f} | max: {:.2f}'.format(X_data[...,4].min(), X_data[...,4].max()))\n",
    "print('heights      - min: {:.2f} | max: {:.2f}'.format(X_data[...,5].min(), X_data[...,5].max()))\n",
    "print('controls     - min: {:.2f} | max: {:.2f}'.format(c_data.min(), c_data.max()))\n",
    "print('pressure_1   - min: {:.2f} | max: {:.2f}'.format(y1_data[...,0].min(), y1_data[...,0].max()))\n",
    "print('saturation_1 - min: {:.2f} | max: {:.2f}'.format(y1_data[...,-1].min(), y2_data[...,-1].max()))\n",
    "print('saturation_2 - min: {:.2f} | max: {:.2f}'.format(y2_data[...,-1].min(), y2_data[...,-1].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.random.choice(range(len(X_data)), 800, replace=False)\n",
    "test_idx  = np.setdiff1d(range(len(X_data)), train_idx)\n",
    "\n",
    "X_train = X_data[train_idx]\n",
    "c_train = c_data[train_idx]\n",
    "y1_train = y1_data[train_idx]\n",
    "y2_train = y2_data[train_idx]\n",
    "\n",
    "X_test = X_data[test_idx]\n",
    "c_test = c_data[test_idx]\n",
    "y1_test = y1_data[test_idx]\n",
    "y2_test = y2_data[test_idx]\n",
    "\n",
    "print('X_train:  {}     | c_train: {}'.format(X_train.shape, c_train.shape))\n",
    "print('y1_train: {} | y2_train: {}'.format(y1_train.shape, y2_train.shape))\n",
    "print('-'*70)\n",
    "print('X_test:  {}     | c_test: {}'.format(X_test.shape, c_test.shape))\n",
    "print('y1_test: {} | y2_test: {}'.format(y1_test.shape, y2_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcite(Layer):\n",
    "    def __init__(self, ratio=4, **kwargs):\n",
    "        super(SqueezeExcite, self).__init__(**kwargs)\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "        self.squeeze = layers.GlobalAveragePooling2D()\n",
    "        self.excite1 = layers.Dense(channels // self.ratio, activation='relu')\n",
    "        self.excite2 = layers.Dense(channels, activation='sigmoid')\n",
    "        super(SqueezeExcite, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        se_tensor = self.squeeze(inputs)\n",
    "        se_tensor = self.excite1(se_tensor)\n",
    "        se_tensor = self.excite2(se_tensor)\n",
    "        se_tensor = layers.Reshape((1, 1, se_tensor.shape[-1]))(se_tensor)\n",
    "        scaled_inputs = layers.Multiply()([inputs, se_tensor])\n",
    "        return layers.Add()([inputs, scaled_inputs])\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(inp, filt, k=3, pad='same', drop=0.1, pool=(2,2)):\n",
    "    _ = layers.SeparableConv2D(filt, k, padding=pad, activity_regularizer=regularizers.l1(1e-6))(inp)\n",
    "    _ = SqueezeExcite()(_)\n",
    "    _ = layers.GroupNormalization(groups=-1)(_)\n",
    "    _ = layers.PReLU()(_)\n",
    "    _ = layers.MaxPooling2D(pool)(_)\n",
    "    #_ = layers.SpatialDropout2D(drop)(_)\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lifting_layer(inp, dim, drop=0.1, nonlinearity='gelu'):\n",
    "    _ = layers.Dense(dim)(inp)\n",
    "    _ = layers.Activation(nonlinearity)(_)\n",
    "    #_ = layers.Dropout(drop)(_)\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurrent_step(inp, filt, res, kern=3, pad='same', drop=0.0, leaky_slope=0.3):\n",
    "    y = layers.ConvLSTM2D(filt, kern, padding=pad)(inp)\n",
    "    y = layers.GroupNormalization(groups=-1)(y)\n",
    "    y = layers.LeakyReLU(leaky_slope)(y)\n",
    "    y = layers.Conv2DTranspose(filt, kern, padding=pad, strides=2)(y)\n",
    "    #y = layers.SpatialDropout2D(drop)(y)\n",
    "    y = layers.Concatenate()([y, res])\n",
    "    y = layers.Conv2D(filt, kern, padding=pad)(y)\n",
    "    y = layers.Activation('sigmoid')(y)\n",
    "    y = tf.expand_dims(y,1)\n",
    "    return y\n",
    "\n",
    "def recurrent_last(inp, filt, kern=3, pad='same', drop=0.0, leaky_slope=0.3, out_channels=2):\n",
    "    y = layers.ConvLSTM2D(filt, kern, padding=pad)(inp)\n",
    "    y = layers.GroupNormalization(groups=-1)(y)\n",
    "    y = layers.LeakyReLU(leaky_slope)(y)\n",
    "    y = layers.Conv2DTranspose(filt, kern, padding=pad, strides=2)(y)\n",
    "    #y = layers.SpatialDropout2D(drop)(y)\n",
    "    y = layers.Conv2D(out_channels, kern, padding=pad)(y)\n",
    "    y = layers.Activation('sigmoid')(y)\n",
    "    y = tf.expand_dims(y, 1)\n",
    "    return y\n",
    "\n",
    "def conditional_recurrent_decoder(z_input, c_input, residuals, rnn_filters=[8,16,64], \n",
    "                                  previous_timestep=None, dropout=0.1, leaky_slope=0.3, out_channels:int=2):\n",
    "    zz = tf.expand_dims(z_input, 1)\n",
    "    cc = tf.expand_dims(c_input, 1)\n",
    "    _ = tf.einsum('bthwc,btc->bthwc', zz, cc)\n",
    "    _ = recurrent_step(_, rnn_filters[0], residuals[0], drop=dropout, leaky_slope=leaky_slope)\n",
    "    _ = recurrent_step(_, rnn_filters[1], residuals[1], drop=dropout, leaky_slope=leaky_slope)\n",
    "    _ = recurrent_last(_, rnn_filters[2], drop=dropout, leaky_slope=leaky_slope, out_channels=out_channels)\n",
    "    if previous_timestep is not None:\n",
    "        _ = layers.Concatenate(axis=1)([previous_timestep, _])\n",
    "    return _\n",
    "\n",
    "def unconditional_recurrent_decoder(z_input, residuals, rnn_filters=[8,16,64], \n",
    "                                    previous_timestep=None, dropout=0.1, leaky_slope=0.3, out_channels:int=1):    \n",
    "    _ = tf.expand_dims(z_input, 1)\n",
    "    _ = recurrent_step(_, rnn_filters[0], residuals[0], drop=dropout, leaky_slope=leaky_slope)\n",
    "    _ = recurrent_step(_, rnn_filters[1], residuals[1], drop=dropout, leaky_slope=leaky_slope)\n",
    "    _ = recurrent_last(_, rnn_filters[2], drop=dropout, leaky_slope=leaky_slope, out_channels=out_channels)\n",
    "    if previous_timestep is not None:\n",
    "        _ = layers.Concatenate(axis=1)([previous_timestep, _])\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(nt=20, hidden=[8, 16, 64], verbose:bool=True):\n",
    "    x_inp = layers.Input(shape=(NX, NY, X_CHANNELS))\n",
    "    c_inp = layers.Input(shape=(nt, 5))\n",
    "\n",
    "    x1 = encoder_layer(x_inp, hidden[0])\n",
    "    x2 = encoder_layer(x1, hidden[1])\n",
    "    x3 = encoder_layer(x2, hidden[2])\n",
    "    zc = lifting_layer(c_inp, hidden[2])\n",
    "    t1 = None\n",
    "    for t in range(nt):\n",
    "        if t==0:\n",
    "            t1 = conditional_recurrent_decoder(x3, zc[:,t], [x2, x1], rnn_filters=hidden)\n",
    "        else:\n",
    "            t1 = conditional_recurrent_decoder(x3, zc[:,t], [x2, x1], rnn_filters=hidden, previous_timestep=t1) \n",
    "            \n",
    "    d_inp = layers.Concatenate()([x_inp, t1[:,-1]])\n",
    "    w1 = encoder_layer(d_inp, hidden[0])\n",
    "    w2 = encoder_layer(w1, hidden[1])\n",
    "    w3 = encoder_layer(w2, hidden[2])\n",
    "    t2 = None\n",
    "    for t in range(nt):\n",
    "        if t==0:\n",
    "            t2 = unconditional_recurrent_decoder(w3, [w2, w1], rnn_filters=hidden)\n",
    "        else:\n",
    "            t2 = unconditional_recurrent_decoder(w3, [w2, w1], rnn_filters=hidden, previous_timestep=t2)\n",
    "    \n",
    "    model = Model(inputs=[x_inp, c_inp], outputs=[t1, t2])\n",
    "    if verbose: print('# parameters: {:,}'.format(model.count_params()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonitorCallback(Callback):\n",
    "    def __init__(self, monitor:int=10):\n",
    "        super(MonitorCallback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch+1) % self.monitor == 0:\n",
    "            print('Epoch: {} | Loss: {:.5f} | Val Loss: {:.5f}'.format(epoch+1, logs['loss'], logs['val_loss']))\n",
    "\n",
    "esCallback = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)\n",
    "mcCallback = keras.callbacks.ModelCheckpoint('pix2vid-opt.keras', monitor='val_accuracy', save_best_only=True)\n",
    "customCBs  = [MonitorCallback(monitor=10), esCallback, mcCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(true, pred, a=0.8, b=0.8):\n",
    "    ssim_loss  = tf.reduce_mean(1.0 - tf.image.ssim(true, pred, max_val=1.0))\n",
    "    mse_loss   = tf.reduce_mean(tf.square(true - pred))\n",
    "    mae_loss   = tf.reduce_mean(tf.abs(true - pred))\n",
    "    pixel_loss = b * mse_loss + (1 - b) * mae_loss\n",
    "    return a * pixel_loss + (1 - a) * ssim_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(nt=20, hidden=[16,64,256])\n",
    "optimizer = keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-6)\n",
    "model.compile(optimizer=optimizer, loss=custom_loss, metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "fit = model.fit(x=[X_train, c_train], y=[y1_train, y2_train],\n",
    "                batch_size       = 8,\n",
    "                epochs           = 10,\n",
    "                validation_split = 0.2,\n",
    "                shuffle          = True,\n",
    "                verbose          = 1)\n",
    "print('-'*30+'\\n'+'Training time: {:.2f} minutes'.format((time()-start)/60))\n",
    "model.save('pix2vid-v2.keras')\n",
    "pd.DataFrame(fit.history).to_csv('pix2vid-v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
