{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "----------------------- VERSION INFO -----------------------\n",
      "Torch version: 2.3.1+cu121 | Torch Built with CUDA? True\n",
      "# Device(s) available: 1, Name(s): NVIDIA GeForce RTX 3080\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from temp import NeuralPix2Vid\n",
    "from neuralop.models import FNO, UNO as UFNO\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from temp import check_torch\n",
    "device = check_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid: (100, 100, 11) | Tops: (100, 100, 11)\n",
      "Grid_ext: (33, 100, 100, 11) | Grid_short_ext: (33, 100, 100, 5)\n"
     ]
    }
   ],
   "source": [
    "sec2year   = 365.25 * 24 * 60 * 60\n",
    "psi2pascal = 6894.76\n",
    "co2_rho    = 686.5266\n",
    "mega       = 1e6\n",
    "\n",
    "n_timesteps = 33\n",
    "nx, ny, nz, nz_short  = 100, 100, 11, 5\n",
    "\n",
    "indexMap = loadmat('data_100_100_11/G_cells_indexMap.mat', simplify_cells=True)['gci']\n",
    "Grid = np.zeros((nx,ny,nz)).flatten(order='F')\n",
    "Grid[indexMap] = 1\n",
    "Grid = Grid.reshape(nx,ny,nz, order='F')\n",
    "Tops = np.load('data_npy_100_100_11/tops_grid.npz')['tops']\n",
    "print('Grid: {} | Tops: {}'.format(Grid.shape, Tops.shape))\n",
    "\n",
    "Grid_short = Grid[:,:,5:10]\n",
    "Grid_ext = np.repeat(np.expand_dims(Grid, 0), 33, axis=0)\n",
    "Grid_short_ext = np.repeat(np.expand_dims(Grid_short, 0), 33, axis=0)\n",
    "print('Grid_ext: {} | Grid_short_ext: {}'.format(Grid_ext.shape, Grid_short_ext.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 4\n",
    "\n",
    "train_idx = np.random.choice(range(1272), size=train_size, replace=False)\n",
    "test_idx  = np.setdiff1d(range(1272), train_idx)\n",
    "\n",
    "xm = np.zeros((len(train_idx), 5, 100,100,5))\n",
    "xc = np.zeros((len(train_idx), n_timesteps, 5))\n",
    "xt = np.zeros((len(train_idx), n_timesteps, 1))\n",
    "yy = np.zeros((len(train_idx), 33, 2, 100,100,5))\n",
    "\n",
    "def apply_mask(x, imap=indexMap, mask_value=0.0):\n",
    "    xx = mask_value*np.ones((nx,ny,nz)).flatten(order='F')\n",
    "    xx[imap] = x.flatten(order='F')[imap]\n",
    "    xx = xx.reshape((nx,ny,nz), order='F')\n",
    "    return xx\n",
    "\n",
    "for i in range(len(train_idx)):\n",
    "    w = np.zeros((100,100,11))\n",
    "    m = np.load('data_npy_100_100_11/inputs_rock_rates_locs_time/x_{}.npz'.format(train_idx[i]))\n",
    "    p = np.expand_dims(apply_mask(m['poro']), 0)[...,5:10] / (0.3)\n",
    "    k = np.expand_dims(apply_mask(m['perm']), 0)[...,5:10] / (3.3)\n",
    "    w[m['wlist'][0,:], m['wlist'][1,:], :] = 1\n",
    "    w = np.expand_dims(apply_mask(w), 0)[...,5:10]\n",
    "    t = np.expand_dims(apply_mask(Tops), 0)[...,5:10] / (Tops.max())\n",
    "    g = np.expand_dims(Grid, 0)[...,5:10]\n",
    "    xm[i] = np.concatenate([p,k,w,t,g], 0)\n",
    "\n",
    "    xc[i] = m['ctrl'] *co2_rho*sec2year/mega/1e3 / (25)\n",
    "    xt[i] = m['time']\n",
    "\n",
    "    dd = np.load('data_npy_100_100_11/outputs_pressure_saturation/y_{}.npz'.format(train_idx[i]))\n",
    "    prm = dd['pressure'][...,5:10] / (psi2pascal)\n",
    "    sam = dd['saturation'][...,5:10]\n",
    "    yy[i,:,0] = np.expand_dims(prm, 0)\n",
    "    yy[i,:,1] = np.expand_dims(sam, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xm torch.Size([4, 5, 100, 100, 5])\n",
      "xc torch.Size([4, 33, 5])\n",
      "xt torch.Size([4, 33, 1])\n",
      "yy torch.Size([4, 33, 2, 100, 100, 5])\n"
     ]
    }
   ],
   "source": [
    "x_m_tensor = torch.tensor(xm, dtype=torch.float32)\n",
    "x_c_tensor = torch.tensor(xc, dtype=torch.float32)\n",
    "x_t_tensor = torch.tensor(xt, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(yy, dtype=torch.float32)\n",
    "print('xm', x_m_tensor.shape)\n",
    "print('xc', x_c_tensor.shape)\n",
    "print('xt', x_t_tensor.shape)\n",
    "print('yy', y_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveONet(nn.Module):\n",
    "    def __init__(self, in_ch:int, out_ch:int=128, n_layers=5, activation=F.gelu, dropout=0.1,\n",
    "                 transformer_num_layers:int=3, \n",
    "                 transformer_nhead:int=4,\n",
    "                 transformer_dim_feedforward:int=1024, \n",
    "                 transformer_activation=F.gelu):\n",
    "        super(AdaptiveONet, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lift = nn.Linear(in_ch, out_ch)\n",
    "        self.norm = nn.LayerNorm(out_ch)\n",
    "        self.layers = nn.ModuleList([nn.Linear(out_ch, out_ch) for _ in range(n_layers)])\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=out_ch, \n",
    "                nhead=transformer_nhead, \n",
    "                dim_feedforward=transformer_dim_feedforward, \n",
    "                activation=transformer_activation,\n",
    "                batch_first=True),\n",
    "            num_layers=transformer_num_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.activation(self.norm(self.lift(x))))\n",
    "        for layer in self.layers:\n",
    "            x = self.dropout(self.activation(self.norm(layer(x))))\n",
    "        x = self.transformer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcite3d(nn.Module):\n",
    "    def __init__(self, channels, ratio=4):\n",
    "        super(SqueezeExcite3d, self).__init__()\n",
    "        self.ratio = ratio\n",
    "        self.squeeze = nn.AdaptiveAvgPool3d(1)\n",
    "        self.excite1 = nn.Linear(channels, channels//ratio)\n",
    "        self.excite2 = nn.Linear(channels//ratio, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _, _ = x.size()\n",
    "        se_tensor = self.squeeze(x).view(b,c)\n",
    "        se_tensor = F.relu(self.excite1(se_tensor))\n",
    "        se_tensor = torch.sigmoid(self.excite2(se_tensor)).view(b,c,1,1,1)\n",
    "        scaled_inputs = x * se_tensor.expand_as(x)\n",
    "        return x + scaled_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialEncoder(nn.Module):\n",
    "    def __init__(self, in_ch:int=5, hidden_ch=[15,45,135], projection_ch:int=128, input_shape=(100,100,5),\n",
    "                 kernel_size=3, padding=1, return_sequence:bool=True):\n",
    "        super(SpatialEncoder, self).__init__()\n",
    "        k, p = kernel_size, padding\n",
    "        h, w, d = input_shape\n",
    "        self.return_sequence = return_sequence\n",
    "        self.conv1 = nn.Conv3d(in_ch, hidden_ch[0], kernel_size=k, padding=p, groups=in_ch)\n",
    "        self.conv2 = nn.Conv3d(hidden_ch[0], hidden_ch[1], kernel_size=k, padding=p, groups=hidden_ch[0])\n",
    "        self.conv3 = nn.Conv3d(hidden_ch[1], hidden_ch[2], kernel_size=k, padding=p, groups=hidden_ch[1])\n",
    "        self.sae1 = SqueezeExcite3d(hidden_ch[0])\n",
    "        self.sae2 = SqueezeExcite3d(hidden_ch[1])\n",
    "        self.sae3 = SqueezeExcite3d(hidden_ch[2])\n",
    "        self.norm1 = nn.GroupNorm(hidden_ch[0], hidden_ch[0])\n",
    "        self.norm2 = nn.GroupNorm(hidden_ch[1], hidden_ch[1])\n",
    "        self.norm3 = nn.GroupNorm(hidden_ch[2], hidden_ch[2])\n",
    "        self.act = nn.GELU()\n",
    "        self.pool1 = nn.AdaptiveMaxPool3d((h//2,w//2,d))\n",
    "        self.pool2 = nn.AdaptiveMaxPool3d((h//4,w//4,d))\n",
    "        self.pool3 = nn.AdaptiveMaxPool3d((h//8,w//8,d))\n",
    "        self.project = nn.Linear(hidden_ch[2], projection_ch)\n",
    "\n",
    "    def forward(self,x):\n",
    "        z1 = self.sae1(self.conv1(x))\n",
    "        z = self.pool1(self.norm1(self.act(z1)))\n",
    "\n",
    "        z2 = self.sae2(self.conv2(z))\n",
    "        z = self.pool2(self.norm2(self.act(z2)))\n",
    "\n",
    "        z3 = self.sae3(self.conv3(z))\n",
    "        z = self.pool3(self.norm3(self.act(z3)))\n",
    "\n",
    "        z4 = self.project(z.permute(0,2,3,4,1)).permute(0,4,1,2,3)\n",
    "        b, c, h, w, d = z4.shape\n",
    "        z = z4.reshape(b, c, h*w*d)\n",
    "        \n",
    "        return z if self.return_sequence else z, (z1,z2,z3,z4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params: 33,490\n",
      "torch.Size([4, 128, 720])\n"
     ]
    }
   ],
   "source": [
    "model = SpatialEncoder()\n",
    "print('# params: {:,}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "zm, hm = model(x_m_tensor)\n",
    "print(zm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params: 1,073,152\n",
      "# params: 1,072,640\n",
      "zc torch.Size([4, 33, 128])\n",
      "zt torch.Size([4, 33, 128])\n",
      "torch.Size([4, 128, 23760])\n"
     ]
    }
   ],
   "source": [
    "branch_c = AdaptiveONet(in_ch=5)\n",
    "branch_t = AdaptiveONet(in_ch=1)\n",
    "print('# params: {:,}'.format(sum(p.numel() for p in branch_c.parameters() if p.requires_grad)))\n",
    "print('# params: {:,}'.format(sum(p.numel() for p in branch_t.parameters() if p.requires_grad)))\n",
    "\n",
    "zc = branch_c(x_c_tensor)\n",
    "zt = branch_t(x_t_tensor)\n",
    "\n",
    "print('zc', zc.shape)\n",
    "print('zt', zt.shape)\n",
    "\n",
    "zb = torch.einsum('bcp,btc->btcp', zm, zc)\n",
    "zz = torch.einsum('btc,btcp->btcp', zt, zb).permute(0,2,1,3).reshape(-1,128,33*720)\n",
    "print(zz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 23760])\n"
     ]
    }
   ],
   "source": [
    "ufno = UFNO(uno_n_modes=[[10],[10],[10],[10],[10]],\n",
    "            uno_out_channels=[128,256,512,256,128],\n",
    "            uno_scalings=[[1],[1],[1],[1],[1]],\n",
    "            in_channels=128, lifting_channels=256, hidden_channels=256, projection_channels=256, out_channels=2,\n",
    "            n_layers=5)\n",
    "\n",
    "print(ufno(zz).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_m_tensor[:-10], x_w_tensor[:-10], \n",
    "                              x_c_tensor[:-10], x_t_tensor[:-10], \n",
    "                              y_tensor[:-10])\n",
    "\n",
    "valid_dataset = TensorDataset(x_m_tensor[-10:], x_w_tensor[-10:], \n",
    "                              x_c_tensor[-10:], x_t_tensor[-10:], \n",
    "                              y_tensor[-10:])\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "validloader = DataLoader(valid_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure as SSIMLoss\n",
    "from torchmetrics.image import SpatialCorrelationCoefficient as SCCLoss\n",
    "from torchmetrics.image import SpectralDistortionIndex as SDILoss\n",
    "from torchmetrics.image import UniversalImageQualityIndex as UIQILoss\n",
    "from torchmetrics.image import VisualInformationFidelity as VIFLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.8, beta=0.7):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.ssim = SSIMLoss()\n",
    "        self.mse  = nn.MSELoss()\n",
    "        self.mae  = nn.L1Loss()\n",
    "        self.alpha = alpha\n",
    "        self.beta  = beta\n",
    "\n",
    "    def forward(self, true, pred):\n",
    "        ridge_loss = self.beta*self.mse(true, pred) + (1-self.beta)*self.mae(true, pred)\n",
    "        visual_loss = 1-self.ssim(true, pred)\n",
    "        total_loss = self.alpha*ridge_loss + (1-self.alpha)*visual_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# parameters: 34,631,418\n"
     ]
    }
   ],
   "source": [
    "model = NeuralPix2Vid().to(device)\n",
    "print('# parameters: {:,}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = CustomLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Padding size should be less than the corresponding input dimension, but got: padding (5, 5) at dimension 2 of input [10, 33, 2, 50000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m yt \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m yh \u001b[38;5;241m=\u001b[39m model(xm, xw, xc, xt)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 15\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43myt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m epoch_train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m, in \u001b[0;36mCustomLoss.forward\u001b[1;34m(self, true, pred)\u001b[0m\n\u001b[0;32m     13\u001b[0m pred \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mreshape(b,t,c,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m ridge_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmse(true, pred) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmae(true, pred)\n\u001b[1;32m---> 15\u001b[0m visual_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#total_loss = self.alpha*ridge_loss + (1-self.alpha)*visual_loss\u001b[39;00m\n\u001b[0;32m     17\u001b[0m true \u001b[38;5;241m=\u001b[39m true\u001b[38;5;241m.\u001b[39mreshape(b,t,c,h,w,d)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchy\\lib\\site-packages\\torchmetrics\\metric.py:303\u001b[0m, in \u001b[0;36mMetric.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_reduce_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchy\\lib\\site-packages\\torchmetrics\\metric.py:372\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[1;32m--> 372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    373\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchy\\lib\\site-packages\\torchmetrics\\metric.py:475\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n\u001b[0;32m    468\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    469\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered different devices in metric calculation (see stacktrace for details).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    470\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m This could be due to the metric class not being on the same device as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m device corresponds to the device of the input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    474\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_on_cpu:\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_move_list_states_to_cpu()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchy\\lib\\site-packages\\torchmetrics\\metric.py:465\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 465\u001b[0m         update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    467\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchy\\lib\\site-packages\\torchmetrics\\image\\ssim.py:131\u001b[0m, in \u001b[0;36mStructuralSimilarityIndexMeasure.update\u001b[1;34m(self, preds, target)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _ssim_check_inputs(preds, target)\n\u001b[1;32m--> 131\u001b[0m similarity_pack \u001b[38;5;241m=\u001b[39m \u001b[43m_ssim_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgaussian_kernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_full_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_contrast_sensitivity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(similarity_pack, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    145\u001b[0m     similarity, image \u001b[38;5;241m=\u001b[39m similarity_pack\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\torchy\\lib\\site-packages\\torchmetrics\\functional\\image\\ssim.py:137\u001b[0m, in \u001b[0;36m_ssim_update\u001b[1;34m(preds, target, gaussian_kernel, sigma, kernel_size, data_range, k1, k2, return_full_image, return_contrast_sensitivity)\u001b[0m\n\u001b[0;32m    135\u001b[0m         kernel \u001b[38;5;241m=\u001b[39m _gaussian_kernel_3d(channel, gauss_kernel_size, sigma, dtype, device)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_h\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreflect\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m     target \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(target, (pad_w, pad_w, pad_h, pad_h), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gaussian_kernel:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Padding size should be less than the corresponding input dimension, but got: padding (5, 5) at dimension 2 of input [10, 33, 2, 50000]"
     ]
    }
   ],
   "source": [
    "epochs, monitor = 101, 10\n",
    "train_loss, valid_loss = [], []\n",
    "for epoch in range(epochs):\n",
    "    # training\n",
    "    epoch_train_loss = []\n",
    "    model.train()\n",
    "    for i, (xm, xw, xc, xt, y) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        xm = xm.to(device)\n",
    "        xw = xw.to(device)\n",
    "        xc = xc.to(device)\n",
    "        xt = xt.to(device)\n",
    "        yt = y.to(device)\n",
    "        yh = model(xm, xw, xc, xt).to(device)\n",
    "        loss = criterion(yt, yh)\n",
    "        optimizer.step()\n",
    "        epoch_train_loss.append(loss.item())\n",
    "    train_loss.append(np.mean(epoch_train_loss))\n",
    "    # validation\n",
    "    model.eval()\n",
    "    epoch_valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        for i, (xmv, xwv, xcv, xtv, yv) in enumerate(validloader):\n",
    "            xmv = xmv.to(device)\n",
    "            xwv = xwv.to(device)\n",
    "            xcv = xcv.to(device)\n",
    "            xtv = xtv.to(device)\n",
    "            yv = yv.to(device)\n",
    "            yhv = model(xmv, xwv, xcv, xtv).to(device)\n",
    "            loss = criterion(yv, yhv)\n",
    "            epoch_valid_loss.append(loss.item())\n",
    "    valid_loss.append(np.mean(epoch_valid_loss))\n",
    "    # print\n",
    "    if epoch % monitor == 0:\n",
    "        print('Epoch: [{}/{}] | Loss: {:.4f} | Valid Loss: {:.4f}'.format(\n",
    "            epoch+1, epochs, train_loss[-1], valid_loss[-1]))\n",
    "\n",
    "torch.save(model.state_dict(), 'neuralpix2vid.pth')\n",
    "pd.DataFrame({'train': train_loss, 'valid': valid_loss}).to_csv('neuralpix2vid_losses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
