{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "----------------------- VERSION INFO -----------------------\n",
      "Torch version: 2.3.1+cu121 | Torch Built with CUDA? True\n",
      "# Device(s) available: 1, Name(s): NVIDIA GeForce RTX 3080\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from temp import NeuralPix2Vid\n",
    "from neuralop.models import FNO\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from temp import check_torch\n",
    "device = check_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid: (100, 100, 11) | Tops: (100, 100, 11)\n",
      "Grid_ext: (33, 100, 100, 11) | Grid_short_ext: (33, 100, 100, 5)\n"
     ]
    }
   ],
   "source": [
    "sec2year   = 365.25 * 24 * 60 * 60\n",
    "psi2pascal = 6894.76\n",
    "co2_rho    = 686.5266\n",
    "mega       = 1e6\n",
    "\n",
    "n_timesteps = 33\n",
    "nx, ny, nz, nz_short  = 100, 100, 11, 5\n",
    "\n",
    "indexMap = loadmat('data_100_100_11/G_cells_indexMap.mat', simplify_cells=True)['gci']\n",
    "Grid = np.zeros((nx,ny,nz)).flatten(order='F')\n",
    "Grid[indexMap] = 1\n",
    "Grid = Grid.reshape(nx,ny,nz, order='F')\n",
    "Tops = np.load('data_npy_100_100_11/tops_grid.npz')['tops']\n",
    "print('Grid: {} | Tops: {}'.format(Grid.shape, Tops.shape))\n",
    "\n",
    "Grid_short = Grid[:,:,5:10]\n",
    "Grid_ext = np.repeat(np.expand_dims(Grid, 0), 33, axis=0)\n",
    "Grid_short_ext = np.repeat(np.expand_dims(Grid_short, 0), 33, axis=0)\n",
    "print('Grid_ext: {} | Grid_short_ext: {}'.format(Grid_ext.shape, Grid_short_ext.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 50\n",
    "\n",
    "train_idx = np.random.choice(range(1272), size=train_size, replace=False)\n",
    "test_idx  = np.setdiff1d(range(1272), train_idx)\n",
    "\n",
    "xm = np.zeros((len(train_idx), 3, 100,100,5))\n",
    "xw = np.zeros((len(train_idx), 2, 5))\n",
    "xc = np.zeros((len(train_idx), n_timesteps, 5))\n",
    "xt = np.zeros((len(train_idx), n_timesteps, 1))\n",
    "yy = np.zeros((len(train_idx), 33, 2, 100,100,5))\n",
    "\n",
    "def apply_mask(x, imap=indexMap, mask_value=0.0):\n",
    "    xx = mask_value*np.ones((nx,ny,nz)).flatten(order='F')\n",
    "    xx[imap] = x.flatten(order='F')[imap]\n",
    "    xx = xx.reshape((nx,ny,nz), order='F')\n",
    "    return xx\n",
    "\n",
    "for i in range(len(train_idx)):\n",
    "    m = np.load('data_npy_100_100_11/inputs_rock_rates_locs_time/x_{}.npz'.format(train_idx[i]))\n",
    "    p = np.expand_dims(apply_mask(m['poro']), 0)[...,5:10] / 0.3\n",
    "    k = np.expand_dims(apply_mask(m['perm']), 0)[...,5:10] / 3.3\n",
    "    t = np.expand_dims(apply_mask(Tops), 0)[...,5:10]      / Tops.max()\n",
    "    xm[i] = np.concatenate([t, p, k], 0)\n",
    "\n",
    "    xw[i] = m['locs']\n",
    "    xc[i] = m['ctrl']\n",
    "    xt[i] = m['time']\n",
    "\n",
    "    dd = np.load('data_npy_100_100_11/outputs_pressure_saturation/y_{}.npz'.format(train_idx[i]))\n",
    "    prm = dd['pressure'][...,5:10]\n",
    "    sam = dd['saturation'][...,5:10]\n",
    "    yy[i,:,0] = np.expand_dims(prm, 0)\n",
    "    yy[i,:,1] = np.expand_dims(sam, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xm torch.Size([50, 3, 100, 100, 5])\n",
      "xw torch.Size([50, 10])\n",
      "xc torch.Size([50, 165])\n",
      "xt torch.Size([50, 33])\n",
      "yy torch.Size([50, 33, 2, 100, 100, 5])\n"
     ]
    }
   ],
   "source": [
    "x_m_tensor = torch.tensor(xm, dtype=torch.float32)\n",
    "x_w_tensor = torch.tensor(xw, dtype=torch.float32).reshape(train_size, -1)\n",
    "x_c_tensor = torch.tensor(xc, dtype=torch.float32).reshape(train_size, -1)\n",
    "x_t_tensor = torch.tensor(xt, dtype=torch.float32).reshape(train_size, -1)\n",
    "y_tensor = torch.tensor(yy, dtype=torch.float32)\n",
    "print('xm', x_m_tensor.shape)\n",
    "print('xw', x_w_tensor.shape)\n",
    "print('xc', x_c_tensor.shape)\n",
    "print('xt', x_t_tensor.shape)\n",
    "print('yy', y_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_m_tensor[:-10], x_w_tensor[:-10], \n",
    "                              x_c_tensor[:-10], x_t_tensor[:-10], \n",
    "                              y_tensor[:-10])\n",
    "\n",
    "valid_dataset = TensorDataset(x_m_tensor[-10:], x_w_tensor[-10:], \n",
    "                              x_c_tensor[-10:], x_t_tensor[-10:], \n",
    "                              y_tensor[-10:])\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "validloader = DataLoader(valid_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure as SSIMLoss\n",
    "from torchmetrics.image import SpatialCorrelationCoefficient as SCCLoss\n",
    "from torchmetrics.image import SpectralDistortionIndex as SDILoss\n",
    "from torchmetrics.image import UniversalImageQualityIndex as UIQILoss\n",
    "from torchmetrics.image import VisualInformationFidelity as VIFLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.8, beta=0.7):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.ssim = SSIMLoss()\n",
    "        self.mse  = nn.MSELoss()\n",
    "        self.mae  = nn.L1Loss()\n",
    "        self.alpha = alpha\n",
    "        self.beta  = beta\n",
    "\n",
    "    def forward(self, true, pred):\n",
    "        ridge_loss = self.beta*self.mse(true, pred) + (1-self.beta)*self.mae(true, pred)\n",
    "        visual_loss = 1-self.ssim(true, pred)\n",
    "        return self.alpha*ridge_loss + (1-self.alpha)*visual_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# parameters: 34,631,418\n"
     ]
    }
   ],
   "source": [
    "model = NeuralPix2Vid().to(device)\n",
    "print('# parameters: {:,}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = CustomLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, monitor = 101, 10\n",
    "train_loss, valid_loss = [], []\n",
    "for epoch in range(epochs):\n",
    "    # training\n",
    "    epoch_train_loss = []\n",
    "    model.train()\n",
    "    for i, (xm, xw, xc, xt, y) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        xm = xm.to(device)\n",
    "        xw = xw.to(device)\n",
    "        xc = xc.to(device)\n",
    "        xt = xt.to(device)\n",
    "        yt = y.to(device)\n",
    "        yh = model(xm, xw, xc, xt).to(device)\n",
    "        loss = criterion(yt, yh)\n",
    "        optimizer.step()\n",
    "        epoch_train_loss.append(loss.item())\n",
    "    train_loss.append(np.mean(epoch_train_loss))\n",
    "    # validation\n",
    "    model.eval()\n",
    "    epoch_valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        for i, (xmv, xwv, xcv, xtv, yv) in enumerate(validloader):\n",
    "            xmv = xmv.to(device)\n",
    "            xwv = xwv.to(device)\n",
    "            xcv = xcv.to(device)\n",
    "            xtv = xtv.to(device)\n",
    "            yv = yv.to(device)\n",
    "            yhv = model(xmv, xwv, xcv, xtv).to(device)\n",
    "            loss = criterion(yv, yhv)\n",
    "            epoch_valid_loss.append(loss.item())\n",
    "    valid_loss.append(np.mean(epoch_valid_loss))\n",
    "    # print\n",
    "    if epoch % monitor == 0:\n",
    "        print('Epoch: [{}/{}] | Loss: {:.4f} | Valid Loss: {:.4f}'.format(\n",
    "            epoch+1, epochs, train_loss[-1], valid_loss[-1]))\n",
    "\n",
    "torch.save(model.state_dict(), 'neuralpix2vid.pth')\n",
    "pd.DataFrame({'train': train_loss, 'valid': valid_loss}).to_csv('neuralpix2vid_losses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
