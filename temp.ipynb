{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "----------------------- VERSION INFO -----------------------\n",
      "Torch version: 2.3.1.post300 | Torch Built with CUDA? True\n",
      "# Device(s) available: 1, Name(s): NVIDIA GeForce RTX 3090\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from temp import NeuralPix2Vid\n",
    "from neuralop.models import FNO, UNO as UFNO\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from temp import check_torch\n",
    "device = check_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec2year   = 365.25 * 24 * 60 * 60\n",
    "psi2pascal = 6894.76\n",
    "co2_rho    = 686.5266\n",
    "mega       = 1e6\n",
    "\n",
    "n_timesteps = 33\n",
    "nx, ny, nz, nz_short  = 100, 100, 11, 5\n",
    "\n",
    "indexMap = loadmat('data_100_100_11/G_cells_indexMap.mat', simplify_cells=True)['gci']\n",
    "Grid = np.zeros((nx,ny,nz)).flatten(order='F')\n",
    "Grid[indexMap] = 1\n",
    "Grid = Grid.reshape(nx,ny,nz, order='F')\n",
    "Tops = np.load('data_npy_100_100_11/tops_grid.npz')['tops']\n",
    "print('Grid: {} | Tops: {}'.format(Grid.shape, Tops.shape))\n",
    "\n",
    "Grid_short = Grid[:,:,5:10]\n",
    "Grid_ext = np.repeat(np.expand_dims(Grid, 0), 33, axis=0)\n",
    "Grid_short_ext = np.repeat(np.expand_dims(Grid_short, 0), 33, axis=0)\n",
    "print('Grid_ext: {} | Grid_short_ext: {}'.format(Grid_ext.shape, Grid_short_ext.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 4\n",
    "\n",
    "train_idx = np.random.choice(range(1272), size=train_size, replace=False)\n",
    "test_idx  = np.setdiff1d(range(1272), train_idx)\n",
    "\n",
    "xm = np.zeros((len(train_idx), 5, 100,100,5))\n",
    "xc = np.zeros((len(train_idx), n_timesteps, 5))\n",
    "xt = np.zeros((len(train_idx), n_timesteps, 1))\n",
    "yy = np.zeros((len(train_idx), 33, 2, 100,100,5))\n",
    "\n",
    "def apply_mask(x, imap=indexMap, mask_value=0.0):\n",
    "    xx = mask_value*np.ones((nx,ny,nz)).flatten(order='F')\n",
    "    xx[imap] = x.flatten(order='F')[imap]\n",
    "    xx = xx.reshape((nx,ny,nz), order='F')\n",
    "    return xx\n",
    "\n",
    "for i in range(len(train_idx)):\n",
    "    w = np.zeros((100,100,11))\n",
    "    m = np.load('data_npy_100_100_11/inputs_rock_rates_locs_time/x_{}.npz'.format(train_idx[i]))\n",
    "    p = np.expand_dims(apply_mask(m['poro']), 0)[...,5:10] / (0.3)\n",
    "    k = np.expand_dims(apply_mask(m['perm']), 0)[...,5:10] / (3.3)\n",
    "    w[m['wlist'][0,:], m['wlist'][1,:], :] = 1\n",
    "    w = np.expand_dims(apply_mask(w), 0)[...,5:10]\n",
    "    t = np.expand_dims(apply_mask(Tops), 0)[...,5:10] / (Tops.max())\n",
    "    g = np.expand_dims(Grid, 0)[...,5:10]\n",
    "    xm[i] = np.concatenate([p,k,w,t,g], 0)\n",
    "\n",
    "    xc[i] = m['ctrl'] *co2_rho*sec2year/mega/1e3 / (25)\n",
    "    xt[i] = m['time']\n",
    "\n",
    "    dd = np.load('data_npy_100_100_11/outputs_pressure_saturation/y_{}.npz'.format(train_idx[i]))\n",
    "    prm = dd['pressure'][...,5:10] / (psi2pascal)\n",
    "    sam = dd['saturation'][...,5:10]\n",
    "    yy[i,:,0] = np.expand_dims(prm, 0)\n",
    "    yy[i,:,1] = np.expand_dims(sam, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_m_tensor = torch.tensor(xm, dtype=torch.float32)\n",
    "x_c_tensor = torch.tensor(xc, dtype=torch.float32)\n",
    "x_t_tensor = torch.tensor(xt, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(yy, dtype=torch.float32)\n",
    "print('xm', x_m_tensor.shape)\n",
    "print('xc', x_c_tensor.shape)\n",
    "print('xt', x_t_tensor.shape)\n",
    "print('yy', y_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveONet(nn.Module):\n",
    "    def __init__(self, in_ch:int, out_ch:int=128, n_layers=5, activation=F.gelu, dropout=0.1,\n",
    "                 transformer_num_layers:int=3, \n",
    "                 transformer_nhead:int=4,\n",
    "                 transformer_dim_feedforward:int=1024, \n",
    "                 transformer_activation=F.gelu):\n",
    "        super(AdaptiveONet, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lift = nn.Linear(in_ch, out_ch)\n",
    "        self.norm = nn.LayerNorm(out_ch)\n",
    "        self.layers = nn.ModuleList([nn.Linear(out_ch, out_ch) for _ in range(n_layers)])\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=out_ch, \n",
    "                nhead=transformer_nhead, \n",
    "                dim_feedforward=transformer_dim_feedforward, \n",
    "                activation=transformer_activation,\n",
    "                batch_first=True),\n",
    "            num_layers=transformer_num_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.activation(self.norm(self.lift(x))))\n",
    "        for layer in self.layers:\n",
    "            x = self.dropout(self.activation(self.norm(layer(x))))\n",
    "        x = self.transformer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcite3d(nn.Module):\n",
    "    def __init__(self, channels, ratio=4):\n",
    "        super(SqueezeExcite3d, self).__init__()\n",
    "        self.ratio = ratio\n",
    "        self.squeeze = nn.AdaptiveAvgPool3d(1)\n",
    "        self.excite1 = nn.Linear(channels, channels//ratio)\n",
    "        self.excite2 = nn.Linear(channels//ratio, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _, _ = x.size()\n",
    "        se_tensor = self.squeeze(x).view(b,c)\n",
    "        se_tensor = F.relu(self.excite1(se_tensor))\n",
    "        se_tensor = torch.sigmoid(self.excite2(se_tensor)).view(b,c,1,1,1)\n",
    "        scaled_inputs = x * se_tensor.expand_as(x)\n",
    "        return x + scaled_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialEncoder(nn.Module):\n",
    "    def __init__(self, in_ch:int=5, hidden_ch=[15,45,135], projection_ch:int=128, input_shape=(100,100,5),\n",
    "                 kernel_size=3, padding=1, return_sequence:bool=True):\n",
    "        super(SpatialEncoder, self).__init__()\n",
    "        k, p = kernel_size, padding\n",
    "        h, w, d = input_shape\n",
    "        self.return_sequence = return_sequence\n",
    "        self.conv1 = nn.Conv3d(in_ch, hidden_ch[0], kernel_size=k, padding=p, groups=in_ch)\n",
    "        self.conv2 = nn.Conv3d(hidden_ch[0], hidden_ch[1], kernel_size=k, padding=p, groups=hidden_ch[0])\n",
    "        self.conv3 = nn.Conv3d(hidden_ch[1], hidden_ch[2], kernel_size=k, padding=p, groups=hidden_ch[1])\n",
    "        self.sae1 = SqueezeExcite3d(hidden_ch[0])\n",
    "        self.sae2 = SqueezeExcite3d(hidden_ch[1])\n",
    "        self.sae3 = SqueezeExcite3d(hidden_ch[2])\n",
    "        self.norm1 = nn.GroupNorm(hidden_ch[0], hidden_ch[0])\n",
    "        self.norm2 = nn.GroupNorm(hidden_ch[1], hidden_ch[1])\n",
    "        self.norm3 = nn.GroupNorm(hidden_ch[2], hidden_ch[2])\n",
    "        self.act = nn.GELU()\n",
    "        self.pool1 = nn.AdaptiveMaxPool3d((h//2,w//2,d))\n",
    "        self.pool2 = nn.AdaptiveMaxPool3d((h//4,w//4,d))\n",
    "        self.pool3 = nn.AdaptiveMaxPool3d((h//8,w//8,d))\n",
    "        self.project = nn.Linear(hidden_ch[2], projection_ch)\n",
    "\n",
    "    def forward(self,x):\n",
    "        z1 = self.sae1(self.conv1(x))\n",
    "        z  = self.pool1(self.act(self.norm1(z1)))\n",
    "\n",
    "        z2 = self.sae2(self.conv2(z))\n",
    "        z  = self.pool2(self.act(self.norm2(z2)))\n",
    "\n",
    "        z3 = self.sae3(self.conv3(z))\n",
    "        z  = self.pool3(self.act(self.norm3(z3)))\n",
    "\n",
    "        z4 = self.project(z.permute(0,2,3,4,1)).permute(0,4,1,2,3)\n",
    "        b, c, h, w, d = z4.shape\n",
    "        z  = z4.reshape(b, c, h*w*d)\n",
    "        \n",
    "        return z if self.return_sequence else z, (z1,z2,z3,z4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "branch_m = SpatialEncoder()\n",
    "branch_c = AdaptiveONet(in_ch=5)\n",
    "branch_t = AdaptiveONet(in_ch=1)\n",
    "print('M# params: {:,}'.format(sum(p.numel() for p in branch_m.parameters() if p.requires_grad)))\n",
    "print('C# params: {:,}'.format(sum(p.numel() for p in branch_c.parameters() if p.requires_grad)))\n",
    "print('T# params: {:,}'.format(sum(p.numel() for p in branch_t.parameters() if p.requires_grad)))\n",
    "\n",
    "zm, hm = branch_m(x_m_tensor)\n",
    "zc = branch_c(x_c_tensor)\n",
    "zt = branch_t(x_t_tensor)\n",
    "\n",
    "zb = torch.einsum('bcp,btc->btcp', zm, zc)\n",
    "zz = torch.einsum('btc,btcp->btcp', zt, zb).permute(0,2,1,3)\n",
    "print(zz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode1, mode2 = 3, 10\n",
    "ufno = UFNO(uno_n_modes=[[mode1,mode2],[mode1,mode2],[mode1,mode2],[mode1,mode2],[mode1,mode2]],\n",
    "            uno_out_channels=[128,256,512,256,128],\n",
    "            uno_scalings=[[1,1],[1,1],[1,1],[1,1],[1,1]],\n",
    "            in_channels=128, lifting_channels=256, hidden_channels=256, projection_channels=256, out_channels=2,\n",
    "            n_layers=5)\n",
    "\n",
    "print('UFNO# params: {:,}'.format(sum(p.numel() for p in ufno.parameters() if p.requires_grad)))\n",
    "zy = ufno(zz)\n",
    "print(zy.shape)\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(8,5), sharex=True, sharey=True)\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        ax = axs[i,j]\n",
    "        d = zy[i,j].detach().numpy()\n",
    "        ax.imshow(d, aspect='auto')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fno = FNO(n_modes=((3,10)),\n",
    "          in_channels=128, lifting_channels=256, hidden_channels=256, projection_channels=256, out_channels=2, \n",
    "          n_layers=5)\n",
    "\n",
    "print('FNO# params: {:,}'.format(sum(p.numel() for p in fno.parameters() if p.requires_grad)))\n",
    "zy = fno(zz)\n",
    "print(zy.shape)\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(8,5), sharex=True, sharey=True)\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        ax = axs[i,j]\n",
    "        d = zy[i,j].detach().numpy()\n",
    "        ax.imshow(d, aspect='auto')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zy = zz.permute(0,2,1,3)\n",
    "print(zy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcQ = nn.Linear(720, 29128)\n",
    "\n",
    "yh = fcQ(zy)\n",
    "print(yh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_m_tensor[:-10], x_c_tensor[:-10], x_t_tensor[:-10], y_tensor[:-10])\n",
    "valid_dataset = TensorDataset(x_m_tensor[-10:], x_c_tensor[-10:], x_t_tensor[-10:], y_tensor[-10:])\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "validloader = DataLoader(valid_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image import StructuralSimilarityIndexMeasure as SSIMLoss\n",
    "from torchmetrics.image import SpatialCorrelationCoefficient as SCCLoss\n",
    "from torchmetrics.image import SpectralDistortionIndex as SDILoss\n",
    "from torchmetrics.image import UniversalImageQualityIndex as UIQILoss\n",
    "from torchmetrics.image import VisualInformationFidelity as VIFLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.8, beta=0.7):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.ssim = SSIMLoss()\n",
    "        self.mse  = nn.MSELoss()\n",
    "        self.mae  = nn.L1Loss()\n",
    "        self.alpha = alpha\n",
    "        self.beta  = beta\n",
    "\n",
    "    def forward(self, true, pred):\n",
    "        ridge_loss = self.beta*self.mse(true, pred) + (1-self.beta)*self.mae(true, pred)\n",
    "        visual_loss = 1-self.ssim(true, pred)\n",
    "        total_loss = self.alpha*ridge_loss + (1-self.alpha)*visual_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralPix2Vid().to(device)\n",
    "print('# parameters: {:,}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = CustomLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, monitor = 101, 10\n",
    "train_loss, valid_loss = [], []\n",
    "for epoch in range(epochs):\n",
    "    # training\n",
    "    epoch_train_loss = []\n",
    "    model.train()\n",
    "    for i, (xm, xw, xc, xt, y) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        xm = xm.to(device)\n",
    "        xw = xw.to(device)\n",
    "        xc = xc.to(device)\n",
    "        xt = xt.to(device)\n",
    "        yt = y.to(device)\n",
    "        yh = model(xm, xw, xc, xt).to(device)\n",
    "        loss = criterion(yt, yh)\n",
    "        optimizer.step()\n",
    "        epoch_train_loss.append(loss.item())\n",
    "    train_loss.append(np.mean(epoch_train_loss))\n",
    "    # validation\n",
    "    model.eval()\n",
    "    epoch_valid_loss = []\n",
    "    with torch.no_grad():\n",
    "        for i, (xmv, xwv, xcv, xtv, yv) in enumerate(validloader):\n",
    "            xmv = xmv.to(device)\n",
    "            xwv = xwv.to(device)\n",
    "            xcv = xcv.to(device)\n",
    "            xtv = xtv.to(device)\n",
    "            yv = yv.to(device)\n",
    "            yhv = model(xmv, xwv, xcv, xtv).to(device)\n",
    "            loss = criterion(yv, yhv)\n",
    "            epoch_valid_loss.append(loss.item())\n",
    "    valid_loss.append(np.mean(epoch_valid_loss))\n",
    "    # print\n",
    "    if epoch % monitor == 0:\n",
    "        print('Epoch: [{}/{}] | Loss: {:.4f} | Valid Loss: {:.4f}'.format(\n",
    "            epoch+1, epochs, train_loss[-1], valid_loss[-1]))\n",
    "\n",
    "torch.save(model.state_dict(), 'neuralpix2vid.pth')\n",
    "pd.DataFrame({'train': train_loss, 'valid': valid_loss}).to_csv('neuralpix2vid_losses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
